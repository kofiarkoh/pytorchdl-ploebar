{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1 torch.Size([4, 1]) torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "print(n_samples, n_features,x.shape, y.shape)\n",
    "\n",
    "input_size = n_features\n",
    "output_size =n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate =0.01\n",
    "n_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Linear Regression Model \n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression,self).__init__()\n",
    "\n",
    "        #define the layers\n",
    "        self.lin = nn.Linear(input_dim,output_dim)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward\n",
    "model =  LinearRegression(input_size,output_size) # uses our own custom class\n",
    "model = nn.Linear(input_size,output_size) # signle layer model from pytorch\n",
    "\n",
    "# loss = MSE\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before traininng f(5) = -0.247\n"
     ]
    }
   ],
   "source": [
    "x_test = torch.tensor([[5]],dtype=torch.float32)\n",
    "print(f\"prediction before traininng f(5) = {model(x_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w=0.196 loss=30.36443710\n",
      "epoch 2: w=0.447 loss=21.10947037\n",
      "epoch 3: w=0.657 loss=14.68740463\n",
      "epoch 4: w=0.832 loss=10.23102951\n",
      "epoch 5: w=0.977 loss=7.13860703\n",
      "epoch 6: w=1.099 loss=4.99260330\n",
      "epoch 7: w=1.200 loss=3.50330377\n",
      "epoch 8: w=1.285 loss=2.46967649\n",
      "epoch 9: w=1.356 loss=1.75223172\n",
      "epoch 10: w=1.415 loss=1.25418186\n",
      "epoch 11: w=1.464 loss=0.90836573\n",
      "epoch 12: w=1.505 loss=0.66818368\n",
      "epoch 13: w=1.539 loss=0.50130004\n",
      "epoch 14: w=1.568 loss=0.38527781\n",
      "epoch 15: w=1.592 loss=0.30454898\n",
      "epoch 16: w=1.613 loss=0.24830988\n",
      "epoch 17: w=1.630 loss=0.20906591\n",
      "epoch 18: w=1.644 loss=0.18161541\n",
      "epoch 19: w=1.656 loss=0.16234964\n",
      "epoch 20: w=1.666 loss=0.14876428\n",
      "epoch 21: w=1.675 loss=0.13912186\n",
      "epoch 22: w=1.682 loss=0.13221638\n",
      "epoch 23: w=1.689 loss=0.12721160\n",
      "epoch 24: w=1.694 loss=0.12352673\n",
      "epoch 25: w=1.699 loss=0.12075913\n",
      "epoch 26: w=1.703 loss=0.11862927\n",
      "epoch 27: w=1.706 loss=0.11694311\n",
      "epoch 28: w=1.709 loss=0.11556602\n",
      "epoch 29: w=1.712 loss=0.11440469\n",
      "epoch 30: w=1.714 loss=0.11339433\n",
      "epoch 31: w=1.716 loss=0.11248986\n",
      "epoch 32: w=1.718 loss=0.11166015\n",
      "epoch 33: w=1.720 loss=0.11088356\n",
      "epoch 34: w=1.721 loss=0.11014487\n",
      "epoch 35: w=1.723 loss=0.10943391\n",
      "epoch 36: w=1.724 loss=0.10874316\n",
      "epoch 37: w=1.725 loss=0.10806775\n",
      "epoch 38: w=1.726 loss=0.10740408\n",
      "epoch 39: w=1.727 loss=0.10674972\n",
      "epoch 40: w=1.729 loss=0.10610303\n",
      "epoch 41: w=1.730 loss=0.10546280\n",
      "epoch 42: w=1.731 loss=0.10482816\n",
      "epoch 43: w=1.731 loss=0.10419866\n",
      "epoch 44: w=1.732 loss=0.10357372\n",
      "epoch 45: w=1.733 loss=0.10295310\n",
      "epoch 46: w=1.734 loss=0.10233665\n",
      "epoch 47: w=1.735 loss=0.10172404\n",
      "epoch 48: w=1.736 loss=0.10111541\n",
      "epoch 49: w=1.737 loss=0.10051060\n",
      "epoch 50: w=1.738 loss=0.09990937\n",
      "epoch 51: w=1.738 loss=0.09931193\n",
      "epoch 52: w=1.739 loss=0.09871799\n",
      "epoch 53: w=1.740 loss=0.09812776\n",
      "epoch 54: w=1.741 loss=0.09754104\n",
      "epoch 55: w=1.742 loss=0.09695781\n",
      "epoch 56: w=1.742 loss=0.09637807\n",
      "epoch 57: w=1.743 loss=0.09580187\n",
      "epoch 58: w=1.744 loss=0.09522909\n",
      "epoch 59: w=1.745 loss=0.09465968\n",
      "epoch 60: w=1.745 loss=0.09409373\n",
      "epoch 61: w=1.746 loss=0.09353118\n",
      "epoch 62: w=1.747 loss=0.09297195\n",
      "epoch 63: w=1.748 loss=0.09241608\n",
      "epoch 64: w=1.748 loss=0.09186354\n",
      "epoch 65: w=1.749 loss=0.09131429\n",
      "epoch 66: w=1.750 loss=0.09076832\n",
      "epoch 67: w=1.751 loss=0.09022561\n",
      "epoch 68: w=1.751 loss=0.08968617\n",
      "epoch 69: w=1.752 loss=0.08914995\n",
      "epoch 70: w=1.753 loss=0.08861693\n",
      "epoch 71: w=1.754 loss=0.08808713\n",
      "epoch 72: w=1.754 loss=0.08756050\n",
      "epoch 73: w=1.755 loss=0.08703694\n",
      "epoch 74: w=1.756 loss=0.08651659\n",
      "epoch 75: w=1.757 loss=0.08599935\n",
      "epoch 76: w=1.757 loss=0.08548513\n",
      "epoch 77: w=1.758 loss=0.08497401\n",
      "epoch 78: w=1.759 loss=0.08446600\n",
      "epoch 79: w=1.760 loss=0.08396098\n",
      "epoch 80: w=1.760 loss=0.08345901\n",
      "epoch 81: w=1.761 loss=0.08295999\n",
      "epoch 82: w=1.762 loss=0.08246399\n",
      "epoch 83: w=1.762 loss=0.08197100\n",
      "epoch 84: w=1.763 loss=0.08148092\n",
      "epoch 85: w=1.764 loss=0.08099376\n",
      "epoch 86: w=1.765 loss=0.08050949\n",
      "epoch 87: w=1.765 loss=0.08002811\n",
      "epoch 88: w=1.766 loss=0.07954966\n",
      "epoch 89: w=1.767 loss=0.07907401\n",
      "epoch 90: w=1.767 loss=0.07860127\n",
      "epoch 91: w=1.768 loss=0.07813130\n",
      "epoch 92: w=1.769 loss=0.07766415\n",
      "epoch 93: w=1.769 loss=0.07719982\n",
      "epoch 94: w=1.770 loss=0.07673828\n",
      "epoch 95: w=1.771 loss=0.07627953\n",
      "epoch 96: w=1.772 loss=0.07582343\n",
      "epoch 97: w=1.772 loss=0.07537004\n",
      "epoch 98: w=1.773 loss=0.07491940\n",
      "epoch 99: w=1.774 loss=0.07447150\n",
      "epoch 100: w=1.774 loss=0.07402626\n",
      "prediction after traininng f(5) = 9.535\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = model(x)\n",
    "\n",
    "    l = loss(y,y_pred)\n",
    "\n",
    "    l.backward() # this calculates the gradients\n",
    "\n",
    "    optimizer.step() # performs optimization step and updates the weights\n",
    "   \n",
    "    optimizer.zero_grad() #empty the gradient\n",
    "    if( epoch % 1 == 0):\n",
    "        [w,b] = model.parameters()\n",
    "        print(f\"epoch {epoch +1}: w={w[0][0].item():.3f} loss={l:.8f}\")\n",
    "\n",
    "\n",
    "print(f\"prediction after traininng f(5) = {model(x_test).item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
